---
title: "XGBoost Project for ADM"
author: "Matthias Quinn"
date: "11/08/2021"
output: 
  html_document: 
    toc: yes
    theme: cerulean
    fig_caption: yes
    df_print: kable
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, fig.width = 8, fig.height = 5)
```

# Exploring and Applying XGBoost

## Abstract:

This project will focus on the famous XGBoost system and its application on moderate and large datasets. Starting with a history of the system, then exploring the algorithm itself, and finally ending with two applications, this project will hopefully provide a framework on which to base further research and applications.

## History:

From Tianqi Chen and Carlos Guestrin in March of 2014, XGBoost has been a monumental system in the field of advanced analytics and machine learning. XGBoost was initially a research project started by Tianqi Chen - and later Carlos Guestrin - at the University of Washington, who presented their paper at SIGKDD Conference in 2016. Since then, version 1.5.0 is available for public usage and has been implemented in more than 5 languages including: C++, Python, R, Java, Scala, and Julia. In addition, the system is available to all modern operating systems, including: Windows, OS X, Linux, and a variety of cloud platforms.

## The XGBoost Algorithm:

Gradient boosting is a machine learning technique for regression and classification problems. The sequential building process of a decision tree usually consists of two loops. The first loop is an outer loop for enumerating the leaf nodes and the second one is an inside of the outer loop that enumerates the features. Instead of sorting the observations of the node by each feature value, you can instead sort the observations first, that is enable global sorting, and then use a scan to decide the best split.

The XGBoost system utilizes a few key system optimizations, including:

1.  **Parallelization:**

    -   The dataset is split into multiple, smaller subsets that are then distributed among multiple CPU cores. Each subset calculates the quantiles in parallel and then each of those data points are pulled in to form a histogram of quantiles.

    -   The weights associated with each quantile is based on the prior probabilities for classification problems.

2.  **Cache-aware Access:**

    -   Essentially, the cached memory within the CPU is the quickest possible way to access data, considering its location in most systems. This is where the $1^{st}$ and $2^{nd}$ derivatives (gradients and hessian matrix) are stored to later calculate the scores for each node and leaf in the tree.

3.  **Regularization:**

    -   The algorithm includes support for both L1 and L2 regularization to prevent over-fitting.

4.  **Exact Greedy Algorithm for Split Finding:**

    -   The XGBoost algorithm utilizes a greedy search approach for finding the optimal splits within the subsets of data, since it is normally impossible to enumerate all of the possible tree structures available.

    -   It should be noted that when dealing with very large datasets, the exact greedy algorithm falls apart since you can't access the cache-aware Hessians and gradients as efficiently.

    -   A greedy algorithm is any procedure that solves a problem by taking the locally-optimal choice at each iteration or stage when asked to present a solution.

------------------------------------------------------------------------

<p style="color:red;font-size:160%">

**Algorithm 1** *Exact Greedy Algorithm for Finding Splits*

</p>

------------------------------------------------------------------------

1.  Let $M_{0}$ denote the *null* model, containing no predictors.

2.  For \$k=0, \dots, p-1\$:

    -   Consider all $p-k$ models that augment model $M_{k}$ with one additional predictor.

    -   Choose the *best* among these $p-k$ models ($\min{(RSS)}$ or $\max{(R^{2})}$.

3.  Select the single best model among the $M_{p}$ possible models using cross validation.

------------------------------------------------------------------------

Boosting fits ensemble models like the following:

$$
\Large f(x) = \sum_{m=0}^{M} f_m(x)
$$

The most common base, weak learners come from other tree algorithms, like decision trees, and the goal of the weak learner is to have high bias and low variance. However, when many of these weak learners are put together, then one gets the added benefit of having a lower bias.

### Important Components for XGBoost to Succeed:

Like many other boosting algorithms, XGBoost relies on 3 keys to success, namely:

1.  A weak, base learner

    -   Simple decision trees are used to fit the training data.

2.  An additive model that reduces the number of failures

    -   By parameterizing each tree that is added to the model, we reduce the residual error and approach the correct direction in the response surface. This methodology is also known as gradient descent.

3.  A loss function

    -   This is typically the MSE for regression problems and is usually the softmax objective function for multi-class classification problems.

## Available Hyper-parameters in Different Implementations:

### Python

### R

### Julia

## Application - Forest Cover Types: {#application---forest-cover-types}

Given forestry data from four wilderness areas in Roosevelt National Forest, classify the patches into one of $7$ cover types, listed below:

1.  Aspen
2.  Cottonwood/Willow
3.  Douglas-fir
4.  Krummholz
5.  Lodgepole Pine
6.  Ponderosa Pine
7.  Spruce/Fir

```{r, "Library Loading"}
library(readxl)
library(dplyr)
library(ggplot2)
library(caret)
library(nnet)
library(xgboost)
library(mlbench)
```

Reading in the forest cover-type dataset.

```{r, "Data Loading"}
X <- read_excel("C:/Users/miqui/OneDrive/Datasets/covtype.xlsx", 
    sheet = "X")
y <- read_excel("C:/Users/miqui/OneDrive/Datasets/covtype.xlsx", 
    sheet = "y")
```

Combine the above two data frames into $1$ for easier access:

```{r, "Merging"}
df <- bind_cols(X, y)
```

Turn the response variable `Cover_Type` into a factor with $7$ levels.

```{r}
df$Cover_Type <- factor(df$Cover_Type)
```

## Exploratory Data Analysis: {#exploratory-data-analysis}

| Variable                           | Type        | Description                                             |
|------------------------------------|-------------|---------------------------------------------------------|
| Elevation                          | Numeric     | Elevation in meters                                     |
| Aspect                             | Numeric     | Aspect in degrees azimuth                               |
| Slope                              | Numeric     | Slope in degrees                                        |
| Horizontal_Distance_To_Hydrology   | Numeric     | Horizontal distance to nearest surface water features   |
| Vertical_Distance_To_Hydrology     | Numeric     | Vertical distance to nearest surface water features     |
| Horizontal_Distance_To_Roadways    | Numeric     | Horizontal distance to the nearest roadway              |
| Hillshade_9am                      | Numeric     | Hillshade index at 9 a.m.                               |
| Hillshade_Noon                     | Numeric     | Hillshade index at noon                                 |
| Hillshade_3pm                      | Numeric     | Hillshade index at 3 p.m.                               |
| Horizontal_Distance_To_Fire_Points | Numeric     | Horizontal distance to nearest wildfire ignition points |
| Wilderness_Area (4 binaries)       | Categorical | Wilderness area designation                             |
| Soil_Type (40 binaries)            | Categorical | Soil type designation                                   |
| Cover_Type (7 types)               | Categorical | Forest cover type designation                           |

: Variables

## Feature Engineering: {#feature-engineering}

It would be nice to have a name associated with each cover type.

```{r}
table(df$Cover_Type)
```

```{r, "Converting Cover Types"}
df <- df %>%
  mutate(CoverName =
           case_when(Cover_Type == 1 ~ "Spruce.fir",
                     Cover_Type == 2 ~ "Lodgepole.Pine",
                     Cover_Type == 3 ~ "Ponderosa.Pine",
                     Cover_Type == 4 ~ "Cottonwood.Willow",
                     Cover_Type == 5 ~ "Aspen",
                     Cover_Type == 6 ~ "Douglas.fir",
                     Cover_Type == 7 ~ "Krummholz",
                     TRUE ~ "Other"))
```

Combining the multiple, binary wilderness area type variables into $1$.

```{r, "Fixing Wilderness Areas"}
df <- df %>%
  mutate(WildernessArea =
           case_when(Wilderness_Area_0 == 1 ~ 0,
                     Wilderness_Area_1 == 1 ~ 1,
                     Wilderness_Area_2 == 1 ~ 2,
                     Wilderness_Area_3 == 1 ~ 3,
                     TRUE ~ NaN))
table(df$WildernessArea)
```

Combining the multiple, binary soil-type variables into $1$.

```{r, "Fixing Soil Types"}
df <- df %>%
  mutate(SoilType =
           case_when(Soil_Type_0 == 1 ~ 0, Soil_Type_1 == 1 ~ 1,
                     Soil_Type_2 == 1 ~ 2, Soil_Type_3 == 1 ~ 3,
                     Soil_Type_4 == 1 ~ 4, Soil_Type_5 == 1 ~ 5,
                     Soil_Type_6 == 1 ~ 6, Soil_Type_7 == 1 ~ 7,
                     Soil_Type_8 == 1 ~ 8, Soil_Type_9 == 1 ~ 9,
                     Soil_Type_10 == 1 ~ 10, Soil_Type_11 == 1 ~ 11,
                     Soil_Type_12 == 1 ~ 12, Soil_Type_13 == 1 ~ 13,
                     Soil_Type_14 == 1 ~ 14, Soil_Type_15 == 1 ~ 15,
                     Soil_Type_16 == 1 ~ 16, Soil_Type_17 == 1 ~ 17,
                     Soil_Type_18 == 1 ~ 18, Soil_Type_19 == 1 ~ 19,
                     Soil_Type_20 == 1 ~ 20, Soil_Type_21 == 1 ~ 21,
                     Soil_Type_22 == 1 ~ 22, Soil_Type_23 == 1 ~ 23,
                     Soil_Type_24 == 1 ~ 24, Soil_Type_25 == 1 ~ 25,
                     Soil_Type_26 == 1 ~ 26, Soil_Type_27 == 1 ~ 27,
                     Soil_Type_28 == 1 ~ 28, Soil_Type_29 == 1 ~ 29,
                     Soil_Type_30 == 1 ~ 30, Soil_Type_31 == 1 ~ 31,
                     Soil_Type_32 == 1 ~ 32, Soil_Type_33 == 1 ~ 33,
                     Soil_Type_34 == 1 ~ 34, Soil_Type_35 == 1 ~ 35,
                     Soil_Type_36 == 1 ~ 36, Soil_Type_37 == 1 ~ 37,
                     Soil_Type_38 == 1 ~ 38, Soil_Type_39 == 1 ~ 39,
                     TRUE ~ NaN))
table(df$SoilType)
```

```{r}
ggplot(df, aes(x=SoilType, fill=CoverName)) + geom_bar(aes(y=(..count..)/sum(..count..))) + ggtitle('Proportion of Observations by Soil Type') + ylab('Proportion') + xlab(NULL) + theme(legend.title=element_blank(), legend.position='bottom') +
scale_x_discrete(breaks = 1:40) + theme(text = element_text(size=16))
```

Euclidean Distance to Hydrology:

```{r}
df$EuclidDistHydro <- (df$Horizontal_Distance_To_Hydrology ** 2 + df$Vertical_Distance_To_Hydrology ** 2) ** 0.50
```

## Modeling:

### Choose the variables that will be fed to the upcoming models. {#choose-the-variables-that-will-be-fed-to-the-upcoming-models.}

```{r}
df <- df %>%
  select(Elevation, Aspect, Slope, Horizontal_Distance_To_Hydrology,
         Vertical_Distance_To_Hydrology, Horizontal_Distance_To_Roadways,
         Hillshade_9am, Hillshade_Noon, Hillshade_3pm, Horizontal_Distance_To_Fire_Points, WildernessArea, SoilType, EuclidDistHydro, Cover_Type,
         CoverName)
```

### Split the main data frame into train-test sets: {#split-the-main-data-frame-into-train-test-sets}

```{r}
set.seed(1234)
inTrain <- createDataPartition(df$Cover_Type,
                                  p = .75, 
                                  list = FALSE, 
                                  times = 1)
training <- df[inTrain, ]
testing <- df[-inTrain, ]
```

### Setting up Cross-Validation: {#setting-up-cross-validation}

```{r, "Setting up Cross-validation"}
trControl <- trainControl(method = "repeatedcv",
                          repeats = 2,
                          verboseIter = TRUE,
                          allowParallel = TRUE,
                          preProcOptions = c("center", "scale"),
                          classProbs = TRUE) # For ROC calculation
```

### Initial Hyper-parameters for XGBoost: {#initial-hyper-parameters-for-xgboost}

```{r, "Creating the Hyperparameters"}
xgbHyperParams <- expand.grid(
                        nrounds = 2,
                        max_depth = c(5, 10),
                        eta = 0.05,
                        gamma = 0.01,
                        colsample_bytree = 0.75,
                        min_child_weight = 0,
                        subsample = 0.5)
```

### Fitting a LDA Model: {#fitting-a-multinomial-logistic-model}

This model should be pretty fast itself.

```{r, "Running the Linear Discriminant Analysis Model"}
set.seed(5678)
ldaMod <- train(CoverName ~ Elevation,
                     data = training,
                     method = "lda",
                     metric = "Accuracy",
                     trControl = trControl)
```

```{r}
ldaMod
```

### Fitting our XGBoost Model: {#fitting-our-xgboost-model}

```{r, "Running the XGBoost Model"}
set.seed(1234)
xgbMod <- train(CoverName ~ Elevation,
                 data = training,
                 method = "xgbTree",
                 metric = "Accuracy",
                 trControl = trControl,
                 tuneGrid = xgbHyperParams,
                 verbose = FALSE)
```

### Examining the Results: {#examining-the-results}

```{r}
xgbMod
```

```{r}
ggplot(xgbMod)
```

## Model Comparison:

```{r}
resamps <- resamples(list(lda = ldaMod, xgb = xgbMod))
summary(resamps)
```

```{r}
xyplot(resamps, what = "BlandAltman")
```

## References: {#references}

[XGBoost: A Scalable Tree Boosting System](https://arxiv.org/abs/1603.02754)

[UCI: Cover Type Data set](https://archive.ics.uci.edu/ml/datasets/Covertype)
