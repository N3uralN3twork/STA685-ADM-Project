{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring and Applying XGBoost\n",
    "\n",
    "## Abstract:\n",
    "\n",
    "This project will focus on the famous XGBoost system and its application on moderate and large datasets. Starting with a history of the system, then exploring the algorithm itself, and finally ending with two applications, this project will hopefully provide a framework on which to base further research and applications.\n",
    "\n",
    "## History:\n",
    "\n",
    "From Tianqi Chen and Carlos Guestrin in March of 2014, XGBoost has been a monumental system in the field of advanced analytics and machine learning. XGBoost was initially a research project started by Tianqi Chen - and later Carlos Guestrin - at the University of Washington, who presented their paper at SIGKDD Conference in 2016. Since then, version 1.5.0 is available for public usage and has been implemented in more than 5 languages including: C++, Python, R, Java, Scala, and Julia. In addition, the system is available to all modern operating systems, including: Windows, OS X, Linux, and a variety of cloud platforms.\n",
    "\n",
    "## The XGBoost Algorithm:\n",
    "\n",
    "Gradient boosting is a machine learning technique for regression and classification problems. The sequential building process of a decision tree usually consists of two loops. The first loop is an outer loop for enumerating the leaf nodes and the second one is an inside of the outer loop that enumerates the features. Instead of sorting the observations of the node by each feature value, you can instead sort the observations first, that is enable global sorting, and then use a scan to decide the best split.\n",
    "\n",
    "The XGBoost system utilizes a few key system optimizations, including:\n",
    "\n",
    "1.  **Parallelization:**\n",
    "\n",
    "    -   The dataset is split into multiple, smaller subsets that are then distributed among multiple CPU cores. Each subset calculates the quantiles in parallel and then each of those data points are pulled in to form a histogram of quantiles.\n",
    "\n",
    "    -   The weights associated with each quantile is based on the prior probabilities for classification problems.\n",
    "\n",
    "2.  **Cache-aware Access:**\n",
    "\n",
    "    -   Essentially, the cached memory within the CPU is the quickest possible way to access data, considering its location in most systems. This is where the $1^{st}$ and $2^{nd}$ derivatives (gradients and hessian matrix) are stored to later calculate the scores for each node and leaf in the tree.\n",
    "\n",
    "3.  **Regularization:**\n",
    "\n",
    "    -   The algorithm includes support for both L1 and L2 regularization to prevent over-fitting.\n",
    "\n",
    "4.  **Exact Greedy Algorithm for Split Finding:**\n",
    "\n",
    "    -   The XGBoost algorithm utilizes a greedy search approach for finding the optimal splits within the subsets of data, since it is normally impossible to enumerate all of the possible tree structures available.\n",
    "\n",
    "    -   It should be noted that when dealing with very large datasets, the exact greedy algorithm falls apart since you can't access the cache-aware Hessians and gradients as efficiently.\n",
    "\n",
    "    -   A greedy algorithm is any procedure that solves a problem by taking the locally-optimal choice at each iteration or stage when asked to present a solution.\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "\n",
    "<p style=\"color:red;font-size:160%\">\n",
    "\n",
    "**Algorithm 1** *Exact Greedy Algorithm for Finding Splits*\n",
    "\n",
    "</p>\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "\n",
    "1.  Let $M_{0}$ denote the *null* model, containing no predictors.\n",
    "\n",
    "2.  For \\$k=0, \\dots, p-1\\$:\n",
    "\n",
    "    -   Consider all $p-k$ models that augment model $M_{k}$ with one additional predictor.\n",
    "\n",
    "    -   Choose the *best* among these $p-k$ models ($\\min{(RSS)}$ or $\\max{(R^{2})}$.\n",
    "\n",
    "3.  Select the single best model among the $M_{p}$ possible models using cross validation.\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "\n",
    "Boosting fits ensemble models like the following:\n",
    "\n",
    "$$\n",
    "\\Large f(x) = \\sum_{m=0}^{M} f_m(x)\n",
    "$$\n",
    "\n",
    "The most common base, weak learners come from other tree algorithms, like decision trees, and the goal of the weak learner is to have high bias and low variance. However, when many of these weak learners are put together, then one gets the added benefit of having a lower bias.\n",
    "\n",
    "### Important Components for XGBoost to Succeed:\n",
    "\n",
    "Like many other boosting algorithms, XGBoost relies on 3 keys to success, namely:\n",
    "\n",
    "1.  A weak, base learner\n",
    "\n",
    "    -   Simple decision trees are used to fit the training data.\n",
    "\n",
    "2.  An additive model that reduces the number of failures\n",
    "\n",
    "    -   By parameterizing each tree that is added to the model, we reduce the residual error and approach the correct direction in the response surface. This methodology is also known as gradient descent.\n",
    "\n",
    "3.  A loss function\n",
    "\n",
    "    -   This is typically the MSE for regression problems and is usually the softmax objective function for multi-class classification problems.\n",
    "\n",
    "## Available Hyper-parameters in Different Implementations:\n",
    "\n",
    "### Python\n",
    "\n",
    "### R\n",
    "\n",
    "### Julia\n",
    "\n",
    "## Application - Forest Cover Types: {#application---forest-cover-types}\n",
    "\n",
    "Given forestry data from four wilderness areas in Roosevelt National Forest, classify the patches into one of $7$ cover types, listed below:\n",
    "\n",
    "1.  Aspen\n",
    "2.  Cottonwood/Willow\n",
    "3.  Douglas-fir\n",
    "4.  Krummholz\n",
    "5.  Lodgepole Pine\n",
    "6.  Ponderosa Pine\n",
    "7.  Spruce/Fir\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "###                    1. Import Libraries and Models                       ###\n",
    "###############################################################################\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import LabelEncoder, OrdinalEncoder, StandardScaler\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "import statsmodels.api as sm\n",
    "from sklearn import metrics\n",
    "from rich import print as rprint\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(action=\"ignore\")\n",
    "\n",
    "pd.set_option('display.max_columns', 20)\n",
    "pd.set_option('display.max_rows', 200)\n",
    "plt.style.use(\"ggplot\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading in the finalized data set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'Elevation', 'Aspect', 'Slope',\n",
       "       'Horizontal_Distance_To_Hydrology', 'Vertical_Distance_To_Hydrology',\n",
       "       'Horizontal_Distance_To_Roadways', 'Hillshade_9am', 'Hillshade_Noon',\n",
       "       'Hillshade_3pm', 'Horizontal_Distance_To_Fire_Points', 'WildernessArea',\n",
       "       'SoilType', 'Cover_Type', 'CoverName'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"df.csv\")\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting the dataset into training and testing sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(columns=[\"Cover_Type\", \"CoverName\"])\n",
    "y = df[[\"Cover_Type\"]]\n",
    "\n",
    "xTrain, xTest, yTrain, yTest = train_test_split(X, y, test_size=0.2, random_state=1234, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the Models to Run Later:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "\n",
    "# Add models as you wish\n",
    "Models = {\n",
    "    \"Logistic Regression\": LogisticRegression(),\n",
    "    \"Decision Tree\": DecisionTreeClassifier(),\n",
    "    \"Random Forest\": RandomForestClassifier(),\n",
    "    \"ADA Boost\": AdaBoostClassifier(),\n",
    "    \"SVClassifier\": SVC(),\n",
    "    \"KNN\": KNeighborsClassifier(),\n",
    "    \"LDA\": LinearDiscriminantAnalysis(),\n",
    "    \"XGBoost\": GradientBoostingClassifier()\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting the Hyperparameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "HyperParams = {}  # Start with an empty dictionary\n",
    "\n",
    "HyperParams.update({\"Multionimial Regression\":\n",
    "                    {\"fit_intercept\": [True],\n",
    "                     \"max_iter\": [100],\n",
    "                     \"verbose\": [1],\n",
    "                     \"n_jobs\": [-1],\n",
    "                     \"penalty\": [\"l1\"],\n",
    "                     \"C\": [1, 10, 50],\n",
    "                     \"solver\": [\"saga\"], # For multi-class classification,\n",
    "                     \"random_state\": [1234]\n",
    "                     }})\n",
    "\n",
    "HyperParams.update({\"Decision Tree\":\n",
    "                    {\"criterion\": [\"gini\", \"entropy\"],\n",
    "                     \"random_state\": [1234]\n",
    "                    }})\n",
    "\n",
    "HyperParams.update({\"Random Forest\": {\n",
    "    \"n_estimators\": [300, 500],\n",
    "    \"max_features\": [\"auto\", \"sqrt\"],\n",
    "    \"n_jobs\": [-1]\n",
    "}})\n",
    "\n",
    "HyperParams.update({\"ADA Boost\":\n",
    "                    {\"n_estimators\": [400],\n",
    "                     \"learning_rate\": [0.001, 0.05, 0.10, 0.5]\n",
    "                     }})\n",
    "\n",
    "HyperParams.update({\"SVClassifier\":\n",
    "                    {\"kernel\": [\"rbf\"],\n",
    "                     \"verbose\": [True],\n",
    "                     \"gamma\": [0.001, 0.0001]\n",
    "                     }})\n",
    "\n",
    "HyperParams.update({\"KNN\":\n",
    "                    {\"n_neighbors\": [3],\n",
    "                     \"algorithm\": [\"auto\"],\n",
    "                     \"n_jobs\": [-1]\n",
    "                     }})\n",
    "\n",
    "HyperParams.update({\"LDA\":\n",
    "                    {\"solver\": [\"eigen\"],\n",
    "                     \"store_covariance\": [False],\n",
    "                     \"shrinkage\": [None]\n",
    "                     }})\n",
    "\n",
    "HyperParams.update({\"XGBoost\":\n",
    "                    {\"max_depth\": [3, 4, 5, 6, 7, 8],\n",
    "                    \"n_estimators\": [500],\n",
    "                    }})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Class to run the Models:\n",
    "\n",
    "The main class for this code comes from David Batista.\n",
    "\n",
    "I simply modified it a bit to my liking.\n",
    "\n",
    "[Source](http://www.davidsbatista.net/blog/2018/02/23/model_optimization/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EstimatorSelectionHelper:\n",
    "\n",
    "    def __init__(self, models, params):\n",
    "        if not set(models.keys()).issubset(set(params.keys())):\n",
    "            missing_params = list(set(models.keys()) - set(params.keys()))\n",
    "            raise ValueError(\n",
    "                \"Some estimators are missing parameters: %s\" % missing_params)\n",
    "        self.models = models\n",
    "        self.params = params\n",
    "        self.keys = models.keys()\n",
    "        self.grid_searches = {}\n",
    "\n",
    "    @staticmethod\n",
    "    def check_rows(X, y):  # My addendum\n",
    "        if X.shape[0] == y.shape[0]:\n",
    "            rprint(\"[bold green]Good[/bold green]: X and Y have the same number of rows\")\n",
    "        else:\n",
    "            rprint(\"[bold red]Bad[/bold red]: X and Y do not have the same number of rows\")\n",
    "\n",
    "    def fit(self, X, y, cv=3, n_jobs=-2, verbose=1, scoring=None, refit=False):\n",
    "        times = []\n",
    "        for key in self.keys:\n",
    "            print(f\"Running GridSearchCV for {key}.\")\n",
    "            start_time = time.time()\n",
    "            \n",
    "            model = self.models[key]\n",
    "            params = self.params[key]\n",
    "            gs = GridSearchCV(model, params, cv=cv, n_jobs=n_jobs,\n",
    "                              verbose=verbose, scoring=scoring, refit=refit,\n",
    "                              return_train_score=True)\n",
    "            gs.fit(X, y)\n",
    "            self.grid_searches[key] = gs\n",
    "            \n",
    "            time_taken = np.round(time.time() - start_time, 4)\n",
    "            rprint(f\"Run complete for [bold blue]{model}[/bold blue]; took [bold green]{np.round(time_taken, 4)}[/bold green] seconds\")\n",
    "            times.append(time_taken)\n",
    "        return times\n",
    "\n",
    "    def score_summary(self, sort_by='mean_score'):\n",
    "        def row(key, scores, params):\n",
    "            d = {\n",
    "                'estimator': key,\n",
    "                'min_score': min(scores),\n",
    "                'max_score': max(scores),\n",
    "                'mean_score': np.mean(scores),\n",
    "                'std_score': np.std(scores),\n",
    "                \"range_score\": (max(scores) - min(scores))  # My own metric\n",
    "            }\n",
    "            return pd.Series({**params, **d})\n",
    "\n",
    "        rows = []\n",
    "        for k in self.grid_searches:\n",
    "            print(k)\n",
    "            params = self.grid_searches[k].cv_results_['params']\n",
    "            scores = []\n",
    "            for i in range(self.grid_searches[k].cv):\n",
    "                key = \"split{}_test_score\".format(i)\n",
    "                r = self.grid_searches[k].cv_results_[key]\n",
    "                scores.append(r.reshape(len(params), 1))\n",
    "\n",
    "            all_scores = np.hstack(scores)\n",
    "            for p, s in zip(params, all_scores):\n",
    "                rows.append((row(k, s, p)))\n",
    "\n",
    "        df = pd.concat(rows, axis=1).T.sort_values([sort_by], ascending=False)\n",
    "\n",
    "        columns = ['estimator', 'min_score', 'mean_score',\n",
    "                   'max_score', 'std_score', \"range_score\"]\n",
    "        columns = columns + [c for c in df.columns if c not in columns]\n",
    "\n",
    "        return df[columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, pass in the dictionary of models and hyperparameters to the class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">Good</span>: X and Y have the same number of rows\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;32mGood\u001b[0m: X and Y have the same number of rows\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pipeline = EstimatorSelectionHelper(Models, HyperParams)\n",
    "pipeline.check_rows(xTrain, yTrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting the models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running GridSearchCV for Logistic Regression.\n",
      "Fitting 2 folds for each of 3 candidates, totalling 6 fits\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Run complete for <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">LogisticRegression()</span>; took <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">96.1046</span> seconds\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Run complete for \u001b[1;34mLogisticRegression\u001b[0m\u001b[1;34m(\u001b[0m\u001b[1;34m)\u001b[0m; took \u001b[1;32m96.1046\u001b[0m seconds\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running GridSearchCV for Decision Tree.\n",
      "Fitting 2 folds for each of 2 candidates, totalling 4 fits\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Run complete for <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">DecisionTreeClassifier()</span>; took <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">14.9905</span> seconds\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Run complete for \u001b[1;34mDecisionTreeClassifier\u001b[0m\u001b[1;34m(\u001b[0m\u001b[1;34m)\u001b[0m; took \u001b[1;32m14.9905\u001b[0m seconds\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running GridSearchCV for Random Forest.\n",
      "Fitting 2 folds for each of 4 candidates, totalling 8 fits\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Run complete for <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">RandomForestClassifier()</span>; took <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">1308.7161</span> seconds\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Run complete for \u001b[1;34mRandomForestClassifier\u001b[0m\u001b[1;34m(\u001b[0m\u001b[1;34m)\u001b[0m; took \u001b[1;32m1308.7161\u001b[0m seconds\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running GridSearchCV for ADA Boost.\n",
      "Fitting 2 folds for each of 4 candidates, totalling 8 fits\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Run complete for <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">AdaBoostClassifier()</span>; took <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">7723.6537</span> seconds\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Run complete for \u001b[1;34mAdaBoostClassifier\u001b[0m\u001b[1;34m(\u001b[0m\u001b[1;34m)\u001b[0m; took \u001b[1;32m7723.6537\u001b[0m seconds\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running GridSearchCV for SVClassifier.\n",
      "Fitting 2 folds for each of 2 candidates, totalling 4 fits\n"
     ]
    }
   ],
   "source": [
    "myMetrics = [\"accuracy\"]\n",
    "pipeline.fit(xTrain, yTrain, cv=2, n_jobs=-2, scoring=myMetrics, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting the Results from the Fitted Models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pipeline.score_summary(sort_by=\"mean_score\")\n",
    "results"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "dc20f31907f5c0096764a8c73a0b20e9c7230fbf5ef8488e45a2156bf72f6ae7"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
